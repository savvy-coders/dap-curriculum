{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analytics\n",
    "\n",
    "## Ingesting Data with Pandas\n",
    "\n",
    "![Python and Pandas!](./images/PythonPandasandDataIngestion.png)\n",
    "\n",
    "## We are going to learn about ...\n",
    "\n",
    "- Reading Data from Excel files\n",
    "- Reading data from SQL databases\n",
    "- Reading data from CSV files\n",
    "- We'll do some Data Wrangling\n",
    "- Pandas in class Practice\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data From Excel into a `DataFrame`:\n",
    "\n",
    "To read an excel file as a DataFrame, use the Python  Pandas `read_excel()` method. You can read different types of Excel file extensions:.xlsx, and .xls.\n",
    "\n",
    "You can read the first sheet, specific sheets, multiple sheets or all sheets. Pandas converts this to the DataFrame structure, which is a tabular like structure.\n",
    "\n",
    "To be able to open Excel files we need to install the module `openpyxl`\n",
    "\n",
    "    --  pip install openpyxl\n",
    "\n",
    "For our example, we will use a file from the resources folder in the curriculum. The filepath to the XLS file is `./resources/sample_winterathletes.xlsx`. \n",
    "\n",
    "We will use the `read_excel` method as mentioned.\n",
    "\n",
    "-   The first parameter is the name of the excel file.\n",
    "-   The sheet_name parameter defines the sheet to be read from the excel file. By default, Pandas will use the first sheet (positionally), unless otherwise specified. To pass multiple sheets use: - `sheet_name=['East', 'West']`\n",
    "\n",
    "The name of the sheet we want to pull from the Excel workbook is `Athletes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import an excel sheet\n",
    "import pandas as pd\n",
    "\n",
    "athletes = pd.read_excel('./resources/sample_winterathletes.xlsx',\n",
    "                    sheet_name='Athletes')\n",
    "athletes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you don’t want to load every column in an Excel file because there are too many columns etc. Use the `usecols=` parameter to select the columns of data you want. EX:-- `usecols=['Customer', 'Sales']`\n",
    "\n",
    "Here is a list of some important parameters that can be used with the `.read_excel()` method:-\n",
    "-   **dtype** – Dict with column name an type.\n",
    "-   **nrows** – How many rows to parse.\n",
    "-   **na_values** – Additional strings to recognize as NA/NaN. \n",
    "-   **keep_default_na** – Whether or not to include the default NaN values when parsing the data. \n",
    "-   **na_filter** – Filters missing values.\n",
    "-   **parse_dates** – Specify the column index you wanted to parse as dates\n",
    "-   **thousands** – Thousands separator for parsing string columns to numeric.\n",
    "-   ***skipfooter*** – Specify how to rows you wanted to skip from the footer.\n",
    "-   **mangle_dupe_cols** – Duplicate columns will be specified as ‘X’, ‘X.1’, …’X.N’, \n",
    "\n",
    "The `.read_excel()` method is a very complex function with many parameters that offer a wide range of useful ways to manipulate Excel input for a Pandas DatFrame. Read all about the [Syntax for `.read_excel()`.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Reading Data From `SQL`  into a `DataFrame`:\n",
    "\n",
    "Pandas has 2 **“read SQL”** methods:-  `pandas.read_sql_query()` and `pandas.read_sql()`. \n",
    "\n",
    "The `.read_sql()` method was added to make it slightly easier to work with SQL data in Pandas. It combines the functionality of two other SQL methods:\n",
    "\n",
    "-   `.read_sql_query()` -- for querying a database and reading the response into a DataFrame once a connection has been setup to the database;\n",
    "-   `.read_sql_table()` -- which allows Pandas to read a whole SQL table from a database into a DataFrame.\n",
    "\n",
    "Syntax of Pandas `.read_sql()`: -\n",
    "\n",
    "```python\n",
    "    # Syntax of read_sql()\n",
    "    pandas.read_sql(sql, con, index_col=None, coerce_float=True, params=None, \n",
    "        parse_dates=None, columns=None, chunksize=None)\n",
    "\n",
    "    # Syntax of read_sql_query()\n",
    "    pandas.read_sql_query(sql, con, index_col=None, coerce_float=True, params=None, \n",
    "        parse_dates=None, chunksize=None, dtype=None)\n",
    "\n",
    "    # Syntax of read_sql_table()\n",
    "    pandas.read_sql_table(table_name, con, schema=None, index_col=None, \n",
    "        coerce_float=True, parse_dates=None, columns=None, chunksize=None)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "We wil use the `pandas.read_sql_query()` as mentioned.\n",
    "\n",
    "The filepath is `./resources/pitchforkDatabase.sqlite`. \n",
    "\n",
    "Use the `sqlite3` library. The name of the table is `artists`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect('./resources/PitchForkDatabase.sqlite')\n",
    "pfDB = pd.read_sql_query('select * from artists',con)\n",
    "pfDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Pandas handles importing SQL data way more eligantly than Python does, the `.read_sql_query()` method is still a complex function with many parameters that offer a wide range of ways to manipulate SQL input to form a Pandas DataFrame. \n",
    "\n",
    "Read all about the [Syntax and use of `.read_sql()`.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html)\n",
    "\n",
    "And here is a very simple and basic article explaining the most useful minimums for working with SQL in Pandas; simple and easy -- [pandas read_sql() method implementation with Examples](https://www.datasciencelearner.com/pandas-read_sql-implementation-examples/)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Reading data from `CSV files` into a `DataFrame`:\n",
    "\n",
    "A simple way to store big data sets is to use CSV files (comma separated files).\n",
    "\n",
    "CSV files contains plain text and is a well know format that can be read by everyone including Pandas.\n",
    "\n",
    "You can open it in Notepad but the format will be off; Use VS Code instead.\n",
    "\n",
    "To access data from the CSV file, we require a function `.read_csv()` that retrieves data in the form of the DataFrame.\n",
    "\n",
    "By default, a `CSV` is separated by commas. But one can use other separators as well. \n",
    "\n",
    "The `pandas.read_csv()` function is not limited to reading the CSV file with default separator (i.e. comma). It can be used for other separators such as `;` or `|` or `:`. \n",
    "\n",
    "To load CSV files with such separators, the `sep=` parameter is used to pass the separator used in the CSV file. Example: --\n",
    "```python\n",
    "    f = pd.read_csv(\"data2.csv\", sep='|')\n",
    "```\n",
    "\n",
    "For our example, we'll use a file from the resources folder in the curriculum. The filepath to the CSV file is `./resources/GREENCOMPUTERS500.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "green = pd.read_csv('./resources/GREENCOMPUTERS500.csv',index_col=0)\n",
    "green"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all about the [Syntax and use of `.read_csv()`.](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Pandas Data Wrangling with a CSV file\n",
    "\n",
    "We will reuse the data file we introduced in the 1st Pandas session. For our example, we will use a file from the resources folder in the curriculum. The filepath to the CSV file is `./resources/data.csv`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and print a CSV file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "# Print Summary of a DataFrame -- 1st 5 & last 5 lines\n",
    "print(df)\n",
    "\n",
    "# Viewing the FIRST 10 rows\n",
    "print(df.head(10))\n",
    "\n",
    "# Viewing the LAST 10 rows\n",
    "print(df.tail(10))\n",
    "\n",
    "# Information about the DataFrame\n",
    "print(df.info())\n",
    "\n",
    "# With this print statement you get the WHOLE DataFrame\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A closer look at the data …\n",
    "\n",
    "Reveals that there are 5 rows in the 'Calories' column without data.\n",
    "\n",
    "Nulls are bad! Nulls are the wrong result when you analyze data.\n",
    "\n",
    "Let’s find the Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding NULLS\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dropping the NULLs -- `.dropna()`**\n",
    "\n",
    "If you studied the output you should have found that the following lins contain NULLs in the 'Calories' column: 17, 27, 91, 118, 141\n",
    "\n",
    "We will use the `.dropna()` method to drop the NULLs.\n",
    "\n",
    "The `dropna()` method removes the rows that contains NULL values.\n",
    "\n",
    "The `dropna()` method returns a new DataFrame object unless the `inplace` parameter is set to `True`, in that case the `dropna()` method does the removing in the original DataFrame instead.\n",
    "\n",
    "> Note: In the example below, this DOES NOT change the original DataFrame BECAUSE we are using `new_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping NULS -- .dropna()\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "new_df = df.dropna()\n",
    "print(new_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This changes the ORIGINAL DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "df.dropna(inplace = True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about the `.dropna()` method ... [pandas.DataFrame.dropna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Replacing Nulls -- `.fillna()`**\n",
    "\n",
    "Often for data integrity sake we do not want to drop NULLs but transform and preserve them. We can replace NULLS with other values, or even calculations.\n",
    "\n",
    "The `fillna()` method replaces the NULL values with a specified value.\n",
    "\n",
    "The `fillna()` method returns a new DataFrame object unless the `inplace` parameter is set to True, in that case the `fillna()` method does the replacing in the original DataFrame instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NULL with 130\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "df.fillna(130, inplace = True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Nulls in Calories column with 130\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "df[\"Calories\"].fillna(130, inplace = True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about the `.fillna()` method ... [pandas.DataFrame.fillna](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Replace Nulls using Mean, Median, Mode**\n",
    "\n",
    "We can replace NULLS with specific calculations to \"fill in\" missing data with valid similar data.\n",
    "\n",
    "A common way to replace empty cells, is to calculate the mean, median or mode value of the column.\n",
    "\n",
    "Pandas uses the `mean()`, `median()` and `mode()` methods to calculate the respective values for a specified column.\n",
    "\n",
    "- **Mean** = the average value\n",
    "- **Median** = the value in the middle, after you have sorted all the values ascending\n",
    "- **Mode** = the value that appears most frequently\n",
    "\n",
    "One of the key points is to decide which technique out of the above-mentioned imputation techniques to use to get the most effective value for the missing values. \n",
    "\n",
    "The goal is to find out which is a better measure of the central tendency of data and use that value for replacing missing values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NULLS with MEAN\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "x = df[\"Calories\"].mean()\n",
    "x = round(x, 2)\n",
    "print(\"the mean ...\", x)\n",
    "\n",
    "df[\"Calories\"].fillna(x, inplace = True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NULLS with MEDIAN\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "x = df[\"Calories\"].median()\n",
    "print(\"the median ...\", x)\n",
    "\n",
    "df[\"Calories\"].fillna(x, inplace =True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NULLS with MODE\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "x = df[\"Calories\"].mode()[0]\n",
    "print(\"the mode ...\", x)\n",
    "\n",
    "df[\"Calories\"].fillna(x, inplace = True)\n",
    "# check columns: 17\t27\t91\t118\t141\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fixing Dates in DataFrames**\n",
    "\n",
    "In most of the big data scenarios , there will be a requirement to fix date issues. It could be necessary to flip a date format, change date formats, or to correct them based on certain criteria, flag incorrect dates, and fix them appropriately.\n",
    "\n",
    "For the next examples we will use a much smaller dataset from the resources folder.\n",
    "\n",
    "The filepath to the CSV file is `./resources/data1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing Dates in DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "print(df.to_string())\n",
    "\n",
    "# drop NULL dates inplace\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.dropna(subset=['Date'], inplace = True)\n",
    "\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing wrong info\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "print(df.to_string())\n",
    "\n",
    "# We want to change line 9 Duration from 60 to be 45\n",
    "df.loc[9, 'Duration'] = 45\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing wrong info in LARGE sets\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "\n",
    "for x in df.index:\n",
    "    if df.loc[x, \"Duration\"] > 120:\n",
    "        df.loc[x, \"Duration\"] = 120\n",
    "\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows in LARGE sets\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data.csv\")\n",
    "print(df.info())\n",
    "\n",
    "for x in df.index:\n",
    "    if df.loc[x, \"Duration\"] > 120:\n",
    "        df.drop(x, inplace = True)\n",
    "\n",
    "print(df.info())\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all Duplicates\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "print(df.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all Duplicates\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(df.duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### More practice with CSV files - Titanic\n",
    "\n",
    "First, we need to gather our data.\n",
    "\n",
    "We can either use the data from our resources directory, or we can import our data from the WEB.\n",
    "\n",
    "The filepath to the CSV file in the curriculum resources folder is `./resources/titanic.csv`.\n",
    "\n",
    "Else, the URL to the data file on the WEB is ... https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
    "\n",
    "You can download that file to your machine, or we will pull that file directly in our code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to gather our data\n",
    "import pandas as pd\n",
    "\n",
    "titanic_data  = pd.read_csv(\"./resources/titanic.csv\")\n",
    "\n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wanted to use the data straight from the web\n",
    "import pandas as pd \n",
    "\n",
    "titanic_data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "\n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you copied the file to your desktop\n",
    "import pandas as pd\n",
    "\n",
    "titanic_data = pd.read_csv(r\"C:\\Users\\User\\Desktop\\DAP2022\\titanic.csv\")\n",
    "\n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizing Data Headers\n",
    "import pandas as pd \n",
    "\n",
    "col_names = [\"Id\", \"Survived\", \n",
    "                \"Passenger Class\", \"Full Name\", \n",
    "                \"Gender\", \"Age\", \"SibSp\", \"Parch\", \n",
    "                \"Ticket Number\", \"Price\", \"Cabin\", \"Station\"] \n",
    "\n",
    "titanic_data = pd.read_csv(r\"./resources/titanic.csv\", names = col_names) \n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipping Rows\n",
    "import pandas as pd \n",
    "\n",
    "col_names = [\"Id\", \"Survived\", \n",
    "                \"Passenger Class\", \"Full Name\", \n",
    "                \"Gender\", \"Age\", \"SibSp\", \"Parch\", \n",
    "                \"Ticket Number\", \"Price\", \"Cabin\", \"Station\"] \n",
    "\n",
    "titanic_data = pd.read_csv(r\"./resources/titanic.csv\", names = col_names, skiprows=[0]) \n",
    "print(titanic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to a new .csv file\n",
    "import pandas as pd \n",
    "\n",
    "col_names = [\"Id\", \"Survived\", \n",
    "                \"Passenger Class\", \"Full Name\", \n",
    "                \"Gender\", \"Age\", \"SibSp\", \"Parch\", \n",
    "                \"Ticket Number\", \"Price\", \"Cabin\", \"Station\"] \n",
    "\n",
    "titanic_data = pd.read_csv(r\"./resources/titanic.csv\", names = col_names, skiprows=[0]) \n",
    "\n",
    "titanic_data.to_csv('use_titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a .csv from scratch\n",
    "import pandas as pd \n",
    "\n",
    "cities = pd.DataFrame([[\"St. Louis\", \"Missouri\"], [\"Atlanta\", \"Georgia\"]], \n",
    "                        columns=[\"City\", \"State\"])\n",
    "\n",
    "cities.to_csv('cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the newly created  .csv file\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('cities.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the file without indexes\n",
    "import pandas as pd \n",
    "\n",
    "cities = pd.DataFrame([[\"St. Louis\", \"Missouri\"], [\"Atlanta\", \"Georgia\"]],\n",
    "                        columns=[\"City\", \"State\"])\n",
    "\n",
    "cities.to_csv('cities.csv', index=False)\n",
    "df = pd.read_csv('cities.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Plotting & Practice with Pandas, Matplotlib and `.csv` files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting directly from a .csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "df.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a scatter plot\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "\n",
    "df.plot(kind = \"scatter\", x = \"Duration\", y = \"Calories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Histogram plot\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"./resources/data1.csv\")\n",
    "\n",
    "df[\"Duration\"].plot(kind ='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue working with Matplotlib in the next session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
