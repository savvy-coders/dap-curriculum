{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbdd9c7",
   "metadata": {},
   "source": [
    "# Data Analytics\n",
    "\n",
    "## BeautifulSoup and Web Scraping\n",
    "\n",
    "The incredible amount of data on the Internet is a rich resource for any field of research or personal interest.\n",
    "\n",
    "To effectively harvest that data, you’ll need to become skilled at web scraping.\n",
    "\n",
    "### What is Web Scraping\n",
    "\n",
    "When a program or script pretends to be a browser and retrieves web pages, looks at those web pages, extracts information, and then looks at more web pages\n",
    "\n",
    "Search engines scrape web pages - we call this “spidering the web” or “web crawling”\n",
    "\n",
    "> Note : Web Scraping is considered as illegal in many cases. It may also cause your IP to be blocked permanently by a website.\n",
    "\n",
    "Some websites don’t like it when automatic scrapers gather their data, while others don’t mind. \n",
    "\n",
    "If you’re scraping a page respectfully for educational purposes, then you’re unlikely to have any problems. Still, it’s a good idea to do some research and make sure that you’re not violating any 'Terms of Service' before you start.\n",
    "\n",
    "### Scraping Web Pages\n",
    "\n",
    "The Python libraries `requests`, `html5lib` and `BeautifulSoup` are powerful tools, perfect for the job of webs craping.\n",
    "\n",
    "Python `requests` is a  module that allows you to send HTTP requests using Python. The HTTP request returns a 'Response Object' with all the response data (content, encoding, status, etc).\n",
    "\n",
    "Once we have accessed the HTML content in a 'Response Object', we need to parse the data. One needs a parser which can create a nested/tree structure of the HTML data. There are many HTML parser libraries available but the most advanced one is `html5lib`.\n",
    "\n",
    "Python `BeautifulSoup` is a  library (from https://www.crummy.com/software/BeautifulSoup/) for pulling data out of `HTML` and `XML` files. This document covers `BeautifulSoup` version 4 that works with Python 3.\n",
    "\n",
    "### Installing `BeautifulSoup`, `html5lib` and `requests`\n",
    "\n",
    "To install these libraries, use PIP.\n",
    "\n",
    "```python\n",
    "    python -m pip install requests\n",
    "    pip install html5lib\n",
    "    pip install beautifulsoup4\n",
    "```\n",
    "\n",
    "### Steps involved in web scraping:\n",
    "\n",
    "1. Send an HTTP request to the URL of the webpage you want to access. The server responds to the request by returning the HTML content of the webpage. For this task, we will use `requests`\n",
    "\n",
    "2. Once we have accessed the HTML content, we are left with the task of parsing the data. Since most of the HTML data is nested, we cannot extract data simply through string processing. One needs a parser which can create a nested/tree structure of the HTML data. For this task, we will use `html5lib`\n",
    "\n",
    "3. Now, we need to navigating and searching the parse tree that we created, i.e. tree traversal. For this task, we will use `BeautifulSoup`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b6a2c",
   "metadata": {},
   "source": [
    "\n",
    "#### **Step 1.) - Accessing the HTML content from a webpage**\n",
    "\n",
    "Import the requests library. Then, specify the URL of the webpage you want to scrape.\n",
    "Send a HTTP request to the specified URL and save the response from the server in a response object called `response`.\n",
    "Now, as print `response.content` to get the raw HTML content of the webpage. It is of ‘string’ type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33416a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import requests\n",
    "\n",
    "# request the URL of the webpage you want to access\n",
    "URL = \"http://www.dr-chuck.com/page1.htm\"\n",
    "# URL = \"https://www.geeksforgeeks.org/data-structures/\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "#  print 'response.content' to get the raw HTML content of the webpage. It's of ‘string’ type\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83c9ca",
   "metadata": {},
   "source": [
    "\n",
    "#### **Step 2.) - Parsing the HTML content**\n",
    "\n",
    "A really nice thing about the BeautifulSoup library is that it is built on the top of the HTML parsing libraries like html5lib, lxml, html.parser, etc. So the BeautifulSoup object and specifying the parser library can be done at the same time.\n",
    "\n",
    "We create a BeautifulSoup object by passing two arguments:\n",
    "- **response.content** : It is the raw HTML content.\n",
    "- **html5lib** : Specifying the HTML parser we want to use.\n",
    "\n",
    "Printing `soup.prettify()` gives the visual representation of the parse tree created from the raw HTML content. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request the URL of the webpage you want to access\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "\n",
    "URL = \"http://www.values.com/inspirational-quotes\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "# parse the response into a readable form\n",
    "soup = bsoup(response.content, 'html5lib')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2aa5d9",
   "metadata": {},
   "source": [
    "#### **Step 3.) - Searching and navigating through the parse tree**\n",
    "\n",
    "Now, we would like to extract some useful data from the HTML content. The soup object contains all the data in the nested structure which could be programmatically extracted. \n",
    "\n",
    "In our example, we are scraping a webpage consisting of some quotes. So, we would like to create a program to save those quotes (and all relevant information about them). \n",
    "\n",
    "Here below is code to do that ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb7df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program to scrape website and save quotes from website\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  as bsoup\n",
    "import csv\n",
    "\n",
    "# request the URL of the webpage you want to access\n",
    "URL = \"http://www.values.com/inspirational-quotes\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "# parse the response into a readable form\n",
    "soup = bsoup(response.content, 'html5lib')\n",
    "# print(soup.prettify())\n",
    "\n",
    "quotes=[] # a list to store quotes\n",
    "\n",
    "# search the response for the HTML container that holds the quotes\n",
    "table = soup.find('div', attrs = {'id':'all_quotes'})\n",
    "# print(table)\n",
    "\n",
    "# iterate the able rows to find each quote info\n",
    "for row in table.findAll('div'):\n",
    "    quote = {}  # create a dictionary for each quote\n",
    "    quote['url'] = \"https:/\" + row.a['href']\n",
    "    quote['lines'] = row.img['alt'].split(\" #\")[0]\n",
    "    quote['theme'] = row.h5.a.text\n",
    "    quote['img'] = row.img['src']\n",
    "    quotes.append(quote)    # attache each quote to the list\n",
    "\n",
    "print(quotes) \n",
    "\n",
    "# save the quotes list of dictionaries into a CSV file\n",
    "filename = 'inspirational_quotes.csv'\n",
    "with open(filename, 'w', newline='') as f:\n",
    "\tw = csv.DictWriter(f,['theme','url','img','lines'])\n",
    "\tw.writeheader()\n",
    "\tfor quote in quotes:\n",
    "\t\tw.writerow(quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128159de",
   "metadata": {},
   "source": [
    "#### Lets analyze the code  \n",
    "\n",
    "- First search through the HTML content of the webpage; print it using `soup.prettify()` method and try to find a pattern or a way to navigate to the quotes.\n",
    "\n",
    "- The quotes are inside a `div` container whose `id` is ‘all_quotes’. So, we find that div element by using the `find()` method :\n",
    "\n",
    "            table = soup.find('div', attrs = {'id':'all_quotes'}) \n",
    "\n",
    "- The first argument is the HTML `div` tag we want; the second argument is a dictionary type element to specify the additional attributes associated with that tag. \n",
    "\n",
    "- The `find()` method returns the first matching element. We can try to print `table.prettify()` to get a sense of what this piece of code does.\n",
    "\n",
    "- Now, in the table element, one can notice that each quote is inside a div container whose class is quote. So, we iterate through each div container with that class.\n",
    "\n",
    "- Finally, we use the `findAll()` method which is similar to the `find()` method in terms of arguments but it returns a list of all matching elements. Each quote is now iterated using a variable called row.\n",
    "\n",
    "- Using the row variable, we find info snippets on each quote to populate a quote dictionary, which is added to the quotes list.\n",
    "\n",
    "- Finally, save the quotes list of dictionaries into a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b8fc58",
   "metadata": {},
   "source": [
    "### A Note on Dynamic Websites\n",
    "\n",
    "In this section we learned how to scrape a static website. \n",
    "\n",
    "Static sites are straightforward to work with because the server sends you an HTML page that already contains all the page information in the response. You can parse that HTML response and immediately begin to pick out the relevant data.\n",
    "\n",
    "On the other hand, with a dynamic website, the server might not send back any HTML at all. Instead, you could receive JavaScript code as a response. This code will look completely different from what you saw when you inspected the page with your browser’s developer tools.\n",
    "\n",
    "Many modern web applications are designed to provide their functionality in collaboration with the clients’ browsers. Instead of sending HTML pages, these apps send JavaScript code that instructs your browser to create the desired HTML. \n",
    "\n",
    "Web apps deliver dynamic content in this way to offload work from the server to the clients’ machines as well as to avoid page reloads and improve the overall user experience.\n",
    "\n",
    "When we use `requests`, we only receive what the server sends back. In the case of a dynamic website, you’ll end up with some JavaScript code instead of HTML. \n",
    "\n",
    "The only way to go from the JavaScript code you received to the content that you’re interested in is to execute the code, just like your browser does. The `requests` library can’t do that for you, but there are other solutions that can.\n",
    "\n",
    "For example, `requests-html` is a project created by the author of the `requests` library that allows you to render JavaScript using syntax that’s similar to the syntax in requests. It also includes capabilities for parsing the data by using Beautiful Soup under the hood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
